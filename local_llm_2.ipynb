{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Obtaining dependency information for gpt4all from https://files.pythonhosted.org/packages/b3/5b/1f4334f54546605efb878129ecde1a6bbab1710c233440fef2888b56b5c8/gpt4all-2.0.2-py3-none-macosx_10_15_universal2.whl.metadata\n",
      "  Downloading gpt4all-2.0.2-py3-none-macosx_10_15_universal2.whl.metadata (892 bytes)\n",
      "Requirement already satisfied: requests in /Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages (from gpt4all) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages (from requests->gpt4all) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages (from requests->gpt4all) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages (from requests->gpt4all) (2023.7.22)\n",
      "Downloading gpt4all-2.0.2-py3-none-macosx_10_15_universal2.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gpt4all\n",
      "Successfully installed gpt4all-2.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all langchain==0.0.342"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download model here https://gpt4all.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\n",
    "    \"./models/gpt4all-falcon-q4_0.gguf\"  # replace with your desired local file path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "1. Justin Bieber was born on March 1, 1994. \n",
      "2. The year he was born was 1994. \n",
      "3. In 1994, the NFL team that won the Super Bowl was the San Francisco 49ers."
     ]
    }
   ],
   "source": [
    "#basic general LLM knowledge\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "output = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/subset.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    " \n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "docs= loader.load()\n",
    "\n",
    "#split in chunks of 2000 characters - max input size for GPT 2000 and a bit\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "#get embeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "\n",
    "persist_directory = 'db/chroma_3/'\n",
    "\n",
    "# Create the vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question in your own words from the \n",
    "context given to you.\n",
    "If questions are asked where there is no relevant context available, please answer from \n",
    "what you know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Human: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"context\",  \"question\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieval \n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": False,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overfitting occurs when the model becomes too complex and fits the training data too closely, resulting in poor generalization performance on new data. This can happen if the model is trained with too much noise or if the model is not able to generalize well enough to new data."
     ]
    }
   ],
   "source": [
    "res = qa(\"What is overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
