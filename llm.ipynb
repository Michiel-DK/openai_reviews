{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4017M  100 4017M    0     0  9499k      0  0:07:13  0:07:13 --:--:-- 10.9M12k      0  0:07:12  0:00:31  0:06:41 10.5M  0     0  9614k      0  0:07:07  0:00:35  0:06:32 10.3MM    0     0  9404k      0  0:07:17  0:00:38  0:06:39 8192k  0:06:36 8984k  0  9254k      0  0:07:24  0:00:45  0:06:39 7086k19k      0  0:07:41  0:00:50  0:06:51 5845k   0  0:07:50  0:01:08  0:06:42 7570k:06:45 6873k7:50  0:01:35  0:06:15 9161k 26 1052M    0     0  9057k      0  0:07:34  0:01:58  0:05:36 10.9M  0     0  9201k      0  0:07:27  0:02:05  0:05:22 11.2M245k      0  0:07:24  0:02:14  0:05:10 9644k   0  0:07:13  0:02:36  0:04:37 10.9M17M   42 1723M    0     0  9331k      0  0:07:20  0:03:09  0:04:11 7757k6 4017M   46 1883M    0     0  9229k      0  0:07:25  0:03:28  0:03:57 8558k    0     0  9057k      0  0:07:34  0:05:45  0:01:49 10.9M 0:01:46 10.6M05:48  0:01:45 10.5MM   85 3431M    0     0  9249k      0  0:07:24  0:06:19  0:01:05 10.9M 0:07:22  0:06:25  0:00:57 11.7M    0  9306k      0  0:07:22  0:06:28  0:00:54 11.3M      0  0:07:15  0:06:58  0:00:17 10.6M\n"
     ]
    }
   ],
   "source": [
    "# We'll start this curl command for later\n",
    "! curl \"https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/llama-7b.ggmlv3.q4_1.bin\" > \"llama.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    " \n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field\\'s methods. \\n\\n\\n== History and relationships to other fields ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".Modern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\\n\\n\\n=== Artificial intelligence ===\\nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.:\\u200a488\\u200aHowever, an increasing emphasis on the logica', metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}),\n",
       " Document(page_content='Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \\nAttention was developed to address the weaknesses of recurrent neural networks, where words in a sentence are slowly processed one at a time.  Recurrent neural networks favor more recent words at the end of a sentence while earlier words fade away in volatile neural activations.  Attention gives all words equal access to any part of a sentence in a faster parallel scheme and no longer suffers the wait time of serial processing.  Earlier uses attached this mechanism to a serial recurrent neural network\\'s language translation system (below), but later uses in Transformers large language models removed the recurrent neural network and relied heavily on the faster parallel attention scheme.\\n\\n\\n== Predecessors ==\\nPredecessors of the mechanism were used in recurrent neural networks which, however, calculated \"soft\" weights sequentially and, at each step, considered the current word and other words within the context window. They were known as multiplicative modules, sigma pi units, and hyper-networks. They have been used in long short-term memory (LSTM) networks, multi-sensory data processing (sound, images, video, and text) in perceivers, fast weight controllers\\'s memory, reasoning tasks in differentiable neural computers, and neural Turing machines\\n\\n\\n== Core Calculations ==\\nThe attention network was designed to identify the highest correlations amongst words within a sentence, assuming that it has learned those patterns from the training corpus.  This correlation is captured in neuronal weights through back-propagation from unsupervised pretraining.\\nThe example below shows how correlations are identified once a network has been trained and has the right weights.  When looking at the word \"that\" in the sentence \"see that girl run\", the network should be able to identify \"girl\" as a highly correlated word.  For simplicity this example focuses on the word \"that\", but in actuality all words receive this treatment in parallel and the resulting soft-weights and context vectors are stacked into matrices for further task- specific use.\\n\\nThe query vector is compared (via dot product) with each word in the keys. This helps the model discover the most relevant word for the query word. In this case \"girl\" was determined to be the most relevant word for \"that\". The result (size 4 in this case) is run through the softmax function, producing a vector of size 4 with probabilities summing to 1. Multiplying this against the value matrix effectively amplifies the signal for the most important words in the sentence and diminishes the signal for less important words.The structure of the input data is captured in the Qw and Kw weights, and the Vw weights express that structure in terms of more meaningful features for the task being trained for.  For this reason, the attention head components are called Query (Q), Key (K), and Value (V)—a loose and possibly misleading analogy with relational database systems.\\nNote that the context vector for \"that\" does not rely on context vectors for the other words; therefore the context vectors of all words can be calculated using the whole matrix X, which includes all the word embeddings, instead of a single word\\'s embedding vector x in the formula above, thus parallelizing the calculations. Now, the softmax can be interpreted as a matrix softmax acting on separate rows.  This is a huge advantage over recurrent networks which must operate sequentially.\\n\\n\\n== A language translation example ==\\nTo build a machine that translates English to French, an attention unit  is grafted to the basic', metadata={'title': 'Attention (machine learning)', 'summary': 'Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \\nAttention was developed to address the weaknesses of recurrent neural networks, where words in a sentence are slowly processed one at a time.  Recurrent neural networks favor more recent words at the end of a sentence while earlier words fade away in volatile neural activations.  Attention gives all words equal access to any part of a sentence in a faster parallel scheme and no longer suffers the wait time of serial processing.  Earlier uses attached this mechanism to a serial recurrent neural network\\'s language translation system (below), but later uses in Transformers large language models removed the recurrent neural network and relied heavily on the faster parallel attention scheme.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)'}),\n",
       " Document(page_content='Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.Most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nSome of the most common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\\n\\n\\n== History ==\\nAt the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine learning spam filter could be used to defeat another machine learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.In 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.Recently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noises. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain\\'s Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state of the art approaches.While adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.\\n\\n\\n=== Examples ===\\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users\\' template galleries that adapt to updated traits over time.\\nResearchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy', metadata={'title': 'Adversarial machine learning', 'summary': 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.Most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nSome of the most common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.', 'source': 'https://en.wikipedia.org/wiki/Adversarial_machine_learning'}),\n",
       " Document(page_content='A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism. It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl, by virtue of the parallelized processing of input sequence.\\nInput text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. Though the transformer paper was published in 2017, the softmax-based attention mechanism was proposed in 2014 for machine translation, and the Fast Weight Controller, similar to a transformer, was proposed in 1992.\\nThis architecture is now used not only in natural language processing and computer vision, but also in audio and multi-modal processing. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (Bidirectional Encoder Representations from Transformers).\\n\\n\\n== Timeline ==\\nIn 1990, the Elman network, using a recurrent neural network, encoded each word in a training set as a vector, called a word embedding, and the whole vocabulary as a vector database, allowing it to perform such tasks as sequence-prediction that are beyond the power of a simple multilayer perceptron. A shortcoming of the static embeddings was that they didn\\'t differentiate between multiple meanings of same-spelt words.\\nIn 1992, the Fast Weight Controller was published by Jürgen Schmidhuber. It learns to answer queries by programming the attention weights of another neural network through outer products of key vectors and value vectors called FROM and TO. The Fast Weight Controller was later shown to be closely related to the Linear Transformer.  The terminology \"learning internal spotlights of attention\" was introduced in 1993.In 1993, the IBM alignment models were used for statistical machine translation.In 1997, a precursor of large language model, using recurrent neural networks, such as long short-term memory, was proposed.In 2001, a one-billion-word large text corpus, scraped from the Internet, referred to as \"very very large\" at the time, was used for word disambiguation.In 2012, AlexNet demonstrated the effectiveness of large neural networks for image recognition, encouraging large artificial neural networks approach instead of older, statistical approaches.In 2014, a 380M-parameter seq2seq model for machine translation using two LSTMs networks was proposed by Sutskever at al. The architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens.In 2014, gating proved to be useful in a 130M-parameter seq2seq model, which used a simplified gated recurrent units (GRUs). Bahdanau et al showed that GRUs are neither better nor worse than gated LSTMs.In 2014,  Bahdanau et al. improved the previous seq2seq model by using an \"additive\" kind of attention mechanism in-between two LSTM networks. It was, however, not yet the parallelizable (scaled \"dot product\") kind of attention, later proposed in the 2017 transformer paper.\\nIn 2015, the relative performance of Global and Local (widowed) attention model architectures were assessed by Luong et al, a mixed attention architecture found to improve on the translations offered by Bahdanau\\'s architecture, while the use of a local attention architecture  reduced translation time. In 2016, Google Translate gradually replaced the older statistical mach', metadata={'title': 'Transformer (machine-learning model)', 'summary': 'A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism. It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl, by virtue of the parallelized processing of input sequence.\\nInput text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. Though the transformer paper was published in 2017, the softmax-based attention mechanism was proposed in 2014 for machine translation, and the Fast Weight Controller, similar to a transformer, was proposed in 1992.\\nThis architecture is now used not only in natural language processing and computer vision, but also in audio and multi-modal processing. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (Bidirectional Encoder Representations from Transformers).\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)'}),\n",
       " Document(page_content='In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\nRobert Schapire\\'s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\\n\\n\\n== Boosting algorithms ==\\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners\\' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\\n\\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation) and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize.\\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.The main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\\n\\n\\n== Object categorization in computer vision ==\\n\\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\\n\\n\\n=== Problem of object categorization ===\\nObject categoriza', metadata={'title': 'Boosting (machine learning)', 'summary': 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\nRobert Schapire\\'s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.', 'source': 'https://en.wikipedia.org/wiki/Boosting_(machine_learning)'}),\n",
       " Document(page_content='Quantum machine learning is the integration of quantum algorithms within machine learning programs.The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\\n\\n\\n== Machine learning with quantum computers ==\\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\\n\\n\\n=== Quantum associative memories and quantum pattern recognition ===\\nAssociative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.\\nTypical classical associative memories store p patterns in the \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.\\nUnfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, \\n  \\n    \\n      \\n        p\\n        ≤\\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle p\\\\leq O(n)}\\n  .\\nQuantum associative memories (in t', metadata={'title': 'Quantum machine learning', 'summary': 'Quantum machine learning is the integration of quantum algorithms within machine learning programs.The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".', 'source': 'https://en.wikipedia.org/wiki/Quantum_machine_learning'}),\n",
       " Document(page_content='The following outline is provided as an overview of and topical guide to machine learning:\\nMachine learning – subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\\n\\n\\n== What type of thing is machine learning? ==\\nAn academic discipline\\nA branch of science\\nAn applied science\\nA subfield of computer science\\nA branch of artificial intelligence\\nA subfield of soft computing\\nApplication of statistics\\n\\n\\n== Branches of machine learning ==\\n\\n\\n=== Subfields of machine learning ===\\nComputational learning theory – studying the design and analysis of machine learning algorithms.\\nGrammar induction\\nMeta-learning\\n\\n\\n=== Cross-disciplinary fields involving machine learning ===\\nAdversarial machine learning\\nPredictive analytics\\nQuantum machine learning\\nRobot learning\\nDevelopmental robotics\\n\\n\\n== Applications of machine learning ==\\nApplications of machine learning\\nBioinformatics\\nBiomedical informatics\\nComputer vision\\nCustomer relationship management –\\nData mining\\nEarth sciences\\nEmail filtering\\nInverted pendulum – balance and equilibrium system.\\nNatural language processing (NLP)\\nNamed Entity Recognition\\nAutomatic summarization\\nAutomatic taxonomy construction\\nDialog system\\nGrammar checker\\nLanguage recognition\\nHandwriting recognition\\nOptical character recognition\\nSpeech recognition\\nText to Speech Synthesis (TTS)\\nSpeech Emotion Recognition (SER)\\nMachine translation\\nQuestion answering\\nSpeech synthesis\\nText mining\\nTerm frequency–inverse document frequency (tf–idf)\\nText simplification\\nPattern recognition\\nFacial recognition system\\nHandwriting recognition\\nImage recognition\\nOptical character recognition\\nSpeech recognition\\nRecommendation system\\nCollaborative filtering\\nContent-based filtering\\nHybrid recommender systems (Collaborative and content-based filtering)\\nSearch engine\\nSearch engine optimization\\nSocial Engineering\\n\\n\\n== Machine learning hardware ==\\nGraphics processing unit\\nTensor processing unit\\nVision processing unit\\n\\n\\n== Machine learning tools ==\\nComparison of deep learning software\\n\\n\\n=== Machine learning frameworks ===\\n\\n\\n==== Proprietary machine learning frameworks ====\\nAmazon Machine Learning\\nMicrosoft Azure Machine Learning Studio\\nDistBelief – replaced by TensorFlow\\n\\n\\n==== Open source machine learning frameworks ====\\nApache Singa\\nApache MXNet\\nCaffe\\nPyTorch\\nmlpack\\nTensorFlow\\nTorch\\nCNTK\\nAccord.Net\\nJax\\nMLJ.jl – A machine learning framework for Julia\\n\\n\\n=== Machine learning libraries ===\\nDeeplearning4j\\nTheano\\nscikit-learn\\nKeras\\n\\n\\n=== Machine learning algorithms ===\\nAlmeida–Pineda recurrent backpropagation\\nALOPEX\\nBackpropagation\\nBootstrap aggregating\\nCN2 algorithm\\nConstructing skill trees\\nDehaene–Changeux model\\nDiffusion map\\nDominance-based rough set approach\\nDynamic time warping\\nError-driven learning\\nEvolutionary multimodal optimization\\nExpectation–maximization algorithm\\nFastICA\\nForward–backward algorithm\\nGeneRec\\nGenetic Algorithm for Rule Set Production\\nGrowing self-organizing map\\nHyper basis function network\\nIDistance\\nK-nearest neighbors algorithm\\nKernel methods for vector output\\nKernel principal component analysis\\nLeabra\\nLinde–Buzo–Gray algorithm\\nLocal outlier factor\\nLogic learning machine\\nLogitBoost\\nManifold alignment\\nMarkov chain Monte Carlo (MCMC)\\nMinimum redundancy feature selection\\nMixture of experts\\nMultiple kernel learning\\nNon-negative matrix factorization\\nOnline machine learning\\nOut-of-bag error\\nPrefrontal cortex basal ganglia working memory\\nPVLV\\nQ-le', metadata={'title': 'Outline of machine learning', 'summary': 'The following outline is provided as an overview of and topical guide to machine learning:\\nMachine learning – subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Outline_of_machine_learning'}),\n",
       " Document(page_content='Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \\nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \\nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\\n\\n\\n== Comparison to the standard approach ==\\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen by the machine learning expert. \\nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.\\n\\n\\n== Targets of automation ==\\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\\n\\nData preparation and ingestion (from raw data and miscellaneous formats)\\nColumn type detection; e.g., boolean, discrete numerical, continuous numerical, or text\\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\\nTask detection; e.g., binary classification, regression, clustering, or ranking\\nFeature engineering\\nFeature selection\\nFeature extraction\\nMeta-learning and transfer learning\\nDetection and handling of skewed data and/or missing values\\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\\nHyperparameter optimization of the learning algorithm and featurization\\nPipeline selection under time, memory, and complexity constraints\\nSelection of evaluation metrics and validation procedures\\nProblem checking\\nLeakage detection\\nMisconfiguration detection\\nAnalysis of obtained results\\nCreating user interfaces and visualizations\\n\\n\\n== See also ==\\nNeural architecture search\\nNeuroevolution\\nSelf-tuning\\nNeural Network Intelligence\\nAutoAI\\nModelOps\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\"Open Source AutoML Tools: AutoGluon, TransmogrifAI, Auto-sklearn, and NNI\". Bizety. 2020-06-16.\\nFerreira, Luís, et al. \"A comparison of AutoML tools for machine learning, deep learning and XGBoost.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021. https://repositorium.sdum.uminho.pt/bitstream/1822/74125/1/automl_ijcnn.pdf', metadata={'title': 'Automated machine learning', 'summary': 'Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \\nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \\nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Automated_machine_learning'}),\n",
       " Document(page_content='Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It may also refer to the corresponding field of study, which develops and studies intelligent machines, or to the intelligent machines themselves.\\nAI technology is widely used throughout industry, government and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning surpassed all previous AI techniques, there was a vast increase in funding and interest.\\nThe various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field\\'s long-term goals.\\nTo solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and many other fields.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning, problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": they became exponentially slower as the problems grew larger.\\nEven humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.\\nAccurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as:\\nobjects, properties, categories and relations between objects;\\n\\nsituations, events, states and time;\\ncauses and effects;\\nknowledge about knowledge (what we know about what other people know);default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\\nAmong the most difficult problems in KR are: the breadth of commonsense knowledge (the set of atom', metadata={'title': 'Artificial intelligence', 'summary': \"Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It may also refer to the corresponding field of study, which develops and studies intelligent machines, or to the intelligent machines themselves.\\nAI technology is widely used throughout industry, government and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning surpassed all previous AI techniques, there was a vast increase in funding and interest.\\nThe various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.\\nTo solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and many other fields.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}),\n",
       " Document(page_content='Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\\n\\n\\n== Definition ==\\nDeep learning is a class of machine learning algorithms that:\\u200a199–200\\u200a uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\\nFrom another angle to view deep learning, deep learning refers to \"computer-simulate\" or \"automate\" human learning processes from a source (e.g., an image of dogs) to a learned object (dogs). Therefore, a notion coined as \"deeper\" learning or \"deepest\" learning makes sense. The deepest learning refers to the fully automatic learning from a source to a final learned object. A deeper learning thus refers to a mixed learning process: a human learning process from a source to a learned semi-object, followed by a computer learning process from the human learned semi-object to a final learned object.\\n\\n\\n== Overview ==\\nMost modern deep learning models are based on multi-layered artificial neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been', metadata={'title': 'Deep learning', 'summary': 'Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Deep_learning'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=text_splitter.split_documents(document)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "#get ada-002 embeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "persist_directory = 'db/chroma/'\n",
    "\n",
    "# Create the vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x1260f09d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=vectordb.similarity_search('what is overfitting?', k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='the implementer is less likely to experience overfitting.', metadata={'source': 'https://en.wikipedia.org/wiki/Support_vector_machine', 'summary': 'In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \\n\\n', 'title': 'Support vector machine'}),\n",
       " Document(page_content='known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak', metadata={'source': 'https://en.wikipedia.org/wiki/Boosting_(machine_learning)', 'summary': 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\nRobert Schapire\\'s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.', 'title': 'Boosting (machine learning)'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from llama.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5809.34 MB (+ 1026.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"llama.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"Q: How large is the earth's diameter? A: 12,756 km B: 13,049 km C: \", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2285.72 ms\n",
      "llama_print_timings:      sample time =    14.11 ms /    20 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time =  2285.67 ms /    15 tokens (  152.38 ms per token)\n",
      "llama_print_timings:        eval time =  1244.78 ms /    19 runs   (   65.51 ms per token)\n",
      "llama_print_timings:       total time =  3982.47 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"Q: How large is the earth's diameter? A: \", \n",
    "             max_tokens=20,  \n",
    "             echo=True)\n",
    "print(output[\"choices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectordb.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python==0.1.65\n",
      "  Downloading llama_cpp_python-0.1.65.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.65)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.65)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/2f/ac/be1f2767b7222347d2fefc18d8d58e9febfd9919190cc6fbd8a4d22d6eab/numpy-1.26.2-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.26.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.65)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.2-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.65-cp310-cp310-macosx_13_0_arm64.whl size=230155 sha256=a79cc1da67be0988b5aa9568aa950e1ded52b03b46f7315f3f70e8538aba7922\n",
      "  Stored in directory: /private/var/folders/3l/70h6qcrd72q0j5x4sscw7qn40000gn/T/pip-ephem-wheel-cache-9iqujhwm/wheels/e0/62/84/21f820209ad725e813c2dd41eeda1a0dcb9184af7022d55c0d\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.20\n",
      "    Uninstalling llama_cpp_python-0.2.20:\n",
      "      Successfully uninstalled llama_cpp_python-0.2.20\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.65 numpy-1.26.2 typing-extensions-4.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python==0.1.65 --force-reinstall --upgrade --no-cache-dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of typing_extensions failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/typing_extensions.py\", line 733, in <module>\n",
      "    class SupportsAbs(Protocol[T_co]):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/typing.py\", line 309, in inner\n",
      "    return cached(*args, **kwds)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/typing.py\", line 1346, in __class_getitem__\n",
      "    return _GenericAlias(cls, params,\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/typing.py\", line 1025, in __init__\n",
      "    self.__parameters__ = _collect_type_vars(params, typevar_types=_typevar_types)\n",
      "NameError: name '_is_unpack' is not defined\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/__init__.py\", line 392, in <module>\n",
      "    _mac_os_check()\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/__init__.py\", line 386, in _mac_os_check\n",
      "    _ = polyfit(x, y, 2, cov=True)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/lib/polynomial.py\", line 667, in polyfit\n",
      "    scale = NX.sqrt((lhs*lhs).sum(axis=0))\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/core/_methods.py\", line 49, in _sum\n",
      "    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "TypeError: float() argument must be a string or a real number, not '_NoValueType'\n",
      "]\n",
      "[autoreload of llama_cpp.llama_cpp failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/llama_cpp/llama_cpp.py\", line 265, in <module>\n",
      "    _lib.llama_model_quantize_default_params.argtypes = []\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/ctypes/__init__.py\", line 387, in __getattr__\n",
      "    func = self.__getitem__(name)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/ctypes/__init__.py\", line 392, in __getitem__\n",
      "    func = self._FuncPtr((name_or_ordinal, self))\n",
      "AttributeError: dlsym(0xb62f7010, llama_model_quantize_default_params): symbol not found. Did you mean: 'llama_context_default_params'?\n",
      "]\n",
      "[autoreload of diskcache.core failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 323, in update_instances\n",
      "    object.__setattr__(ref, \"__class__\", new)\n",
      "TypeError: __class__ assignment: 'Constant' object layout differs from 'Constant'\n",
      "]\n",
      "[autoreload of llama_cpp.llama failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n",
      "[autoreload of numpy.testing._private.utils failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/testing/_private/utils.py\", line 57, in <module>\n",
      "    HAS_LAPACK64 = numpy.linalg._umath_linalg._ilp64\n",
      "AttributeError: module 'numpy.linalg' has no attribute '_umath_linalg'\n",
      "]\n",
      "[autoreload of numpy.testing failed: Traceback (most recent call last):\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/testing/__init__.py\", line 17, in <module>\n",
      "    _private.utils.__all__ + ['TestCase', 'overrides']\n",
      "AttributeError: module 'numpy.testing._private' has no attribute 'utils'\n",
      "]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/llms/llamacpp.py:143\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaGrammar' from 'llama_cpp' (/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/llama_cpp/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaCpp\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m LlamaCpp(model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/pydantic/v1/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[39m=\u001b[39m validator(cls_, values)\n\u001b[1;32m   1103\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[39m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/llms/llamacpp.py:145\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    146\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama-cpp-python library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install the llama-cpp-python library to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    151\u001b[0m model_path \u001b[39m=\u001b[39m values[\u001b[39m\"\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    152\u001b[0m model_param_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrope_freq_scale\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrope_freq_base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    169\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "LlamaCpp(model_path=\"llama.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'numpy.testing' from '/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/numpy/testing/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "#QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "\n",
    "# Call OpenAI API via LangChain\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#input_key=\"question\",\n",
    "def generate_response(query,chat_history):\n",
    "    if query:\n",
    "        my_qa = ConversationalRetrievalChain.from_llm(Llama(model_path=\"llama.bin\"), retriever, QA_PROMPT, verbose=True, memory=memory)\n",
    "        result = my_qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'llama_cpp.llama_cpp' has no attribute 'llama_init_from_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_response(\u001b[39m'\u001b[39;49m\u001b[39mwhat is overfitting\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(query,chat_history):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m query:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         my_qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(Llama(model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m), retriever, QA_PROMPT, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, memory\u001b[39m=\u001b[39mmemory)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X52sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         result \u001b[39m=\u001b[39m my_qa({\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: query, \u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m: chat_history})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/llama_cpp/llama.py:158\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, verbose)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    157\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLlamaDiskCache.__setitem__: set\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m--> 158\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_size \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapacity_bytes \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    159\u001b[0m     key_to_remove \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache))\n\u001b[1;32m    160\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache[key_to_remove]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'llama_cpp.llama_cpp' has no attribute 'llama_init_from_file'"
     ]
    }
   ],
   "source": [
    "generate_response('what is overfitting', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  2285.72 ms\n",
      "llama_print_timings:      sample time =    18.58 ms /    26 runs   (    0.71 ms per token)\n",
      "llama_print_timings: prompt eval time =  5677.37 ms /    30 tokens (  189.25 ms per token)\n",
      "llama_print_timings:        eval time =  1490.82 ms /    25 runs   (   59.63 ms per token)\n",
      "llama_print_timings:       total time =  7765.57 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-e3a66116-5f15-4db0-9ef6-cd40e2cfbdde',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1701437507,\n",
       " 'model': 'llama.bin',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'This is a picture of a girl with long hair, she has a pink dress and a blue hat. '},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 31, 'completion_tokens': 26, 'total_tokens': 57}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39;49mprompt, llm\u001b[39m=\u001b[39;49mllm)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/llms/llamacpp.py:143\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlamaGrammar' from 'llama_cpp' (/Users/michieldekoninck/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/llama_cpp/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m LlamaCpp(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.75\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     callback_manager\u001b[39m=\u001b[39;49mcallback_manager,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Verbose is required to pass to the callback manager\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/pydantic/v1/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[39m=\u001b[39m validator(cls_, values)\n\u001b[1;32m   1103\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[39m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/llms/llamacpp.py:145\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    146\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama-cpp-python library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install the llama-cpp-python library to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    151\u001b[0m model_path \u001b[39m=\u001b[39m values[\u001b[39m\"\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    152\u001b[0m model_param_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrope_freq_scale\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrope_freq_base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    169\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"llama.bin\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michieldekoninck/code/Michiel-DK/openai_reviews/llm.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39;49mfrom_llm(llm, vectordb\u001b[39m.\u001b[39;49mas_retriever(), return_source_documents\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:360\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convenience method to load chain from LLM and retriever.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[39mThis provides some logic to create the `question_generator` chain\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39m        ConversationalRetrievalChain\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m combine_docs_chain_kwargs \u001b[39m=\u001b[39m combine_docs_chain_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 360\u001b[0m doc_chain \u001b[39m=\u001b[39m load_qa_chain(\n\u001b[1;32m    361\u001b[0m     llm,\n\u001b[1;32m    362\u001b[0m     chain_type\u001b[39m=\u001b[39;49mchain_type,\n\u001b[1;32m    363\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    364\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    365\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcombine_docs_chain_kwargs,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    368\u001b[0m _llm \u001b[39m=\u001b[39m condense_question_llm \u001b[39mor\u001b[39;00m llm\n\u001b[1;32m    369\u001b[0m condense_question_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[1;32m    370\u001b[0m     llm\u001b[39m=\u001b[39m_llm,\n\u001b[1;32m    371\u001b[0m     prompt\u001b[39m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    372\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    373\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    374\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m chain_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unsupported chain type: \u001b[39m\u001b[39m{\u001b[39;00mchain_type\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould be one of \u001b[39m\u001b[39m{\u001b[39;00mloader_mapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[39mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[1;32m    250\u001b[0m     llm, verbose\u001b[39m=\u001b[39;49mverbose, callback_manager\u001b[39m=\u001b[39;49mcallback_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    251\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py:73\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_stuff_chain\u001b[39m(\n\u001b[1;32m     64\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     65\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     71\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StuffDocumentsChain:\n\u001b[1;32m     72\u001b[0m     _prompt \u001b[39m=\u001b[39m prompt \u001b[39mor\u001b[39;00m stuff_prompt\u001b[39m.\u001b[39mPROMPT_SELECTOR\u001b[39m.\u001b[39mget_prompt(llm)\n\u001b[0;32m---> 73\u001b[0m     llm_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[1;32m     74\u001b[0m         llm\u001b[39m=\u001b[39;49mllm,\n\u001b[1;32m     75\u001b[0m         prompt\u001b[39m=\u001b[39;49m_prompt,\n\u001b[1;32m     76\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     77\u001b[0m         callback_manager\u001b[39m=\u001b[39;49mcallback_manager,\n\u001b[1;32m     78\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m     \u001b[39m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[1;32m     82\u001b[0m         llm_chain\u001b[39m=\u001b[39mllm_chain,\n\u001b[1;32m     83\u001b[0m         document_variable_name\u001b[39m=\u001b[39mdocument_variable_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/langchain/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
